1) remove quotation marks from dataset by opening all the csv files with notepad++, then press ctrl+H, replaceall in all documents (make sure no other documents are open at that time!) " and ; by empty string
2) remove beforeunload and polling identifiers
2a) removed wrong semicolons ':' (semicolons that started at the front of the time)
3) count the occurrences of main domains (for each load identifier) -> determine whether a Markov Model would be appropriate (sparse matrix or not?)
3a) clean data (filter ads, widgets, useless loads and clicks)
3b) remove document types out of path
3c) for every domain in the load:
      if path has depth 0, remove
      if path has depth > 0 but occurs only once, remove
3d) generate ground truth based on remaining loads:
     - for all users
     - per user (only if data is at least 30 rows large; otherwise that user has too little data)
4*) determine training/test split (use k-folds from scikit on one hand; use manual splitting on the other hand)
    For the manual splitting, try 50/50; 60/40; 80/20 for training/test split
	pros/cons of k-fold vs manually: random distribution, so no bias. But as a user we know what to look for, so we can better adjust the training dataset to our needs. However, this is labor intensive and biased.
5*) apply Markov Models with different thresholds (< 10; < 20)
6*) after results are obtained, try to come up with a good explanation with the clicks (how did users get to this page?)




NOTES:

-don't add user id since we are learning over the entire dataset. Generalize
  -> we may reason that when applied to a particular user, that it's only personalized to the user since only the user's history is used to learn. (but this may be what we want!)
-pauses of 10 mins or longer counts as a new session: we reasoned that the users were conscious about generating clicks, so a break of 10 minutes or longer is rare.
-use info from the web navigation literature!!  user id, session id, ...:
   - first perform data cleaning (remove useless information)
   - identify users (already done for us, since the number of the csv files indicates the user id -> might have to add to the array)
   - identify sessions (using 'timeout': once the period between two clicks reaches a certain threshold, then a new session starts)
   - create data model

- possible to test different things:
    - entire dataset vs per user
    - training/test splits (see above)
    - markov model / other technique