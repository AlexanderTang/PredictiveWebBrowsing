1) adjust datasets:
    -split timestamp into dates and time
    -remove beforeunload
2) split training and test data sets
3) apply markov chains (do this for cross validation: mix up training and data sets again)
4) interpret results


criteria:
- evaluate over all data: data set per user is too limited to get accurate results; the freedom in choosing websites is too wide for per end page to supply reliable results (verify this by observing the website counts in the total data sets)
- for the evaluation, take the frequency of the website into account: low confidence interval means a high accuracy doesn't have as much meaning
- computational efficiency: 
  - possibly only look into certain depths to reduce computation time; this could even improve accuracy by avoiding overfitting
- cut hashes from urls